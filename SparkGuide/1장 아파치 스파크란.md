
## 1장 아파치 스파크란

### 스파크 기능 구성
<img src="./image/1-1.png" width="600">

### 컴퓨팅 엔진
```commandline
- 스파크는 저장소 시스템의 데이터를 연산하는 역할을 수행
- 영구 저장소 역할은 수행하지 않으며 대신에 AWS S3, Hadoop, 카산드라, 카프카 등의 저장소를 지원
```

### 스파크 설치하기
파이썬 콘솔 실행하기
SQL 콘솔 실행하기


## 2장 스파크 간단히 살펴보기

## 스파크 기본 아키텍처
```commandline
클러스터
- 클러스터는 여러 컴퓨터의 자원을 모아 하나의 컴퓨터 처럼 사용
- 스파크는 클러스터에서 작업을 조율할 수 있는 역할을 하는 프레임워크
```

```commandline
드라이버 프로세스
- 클러스터 노드 중 하나에서 실행되며 main() 함수를 실행
- 필수적으로 존재

익스큐터 프로세스
- 드라이버가 할당한 작업을 수행 후, 드라이버 노드에 보고하는 두가지 역할 수행
```
<img src="./image/1-2.png" width="500">

클러스터 모드 실행 예제
```commandline
from pyspark.sql import SparkSession

spark = SparkSession.builder \
    .appName("MySparkApp") \  # 애플리케이션 이름 설정
    .master("spark://<master-hostname>:<master-port>") \  # 마스터 노드의 주소 설정
    .config("spark.executor.instances", "4") \  # 실행할 Executor 인스턴스 수 설정
    .config("spark.executor.cores", "4") \  # 각 Executor의 코어 수 설정
    .getOrCreate()
```

클라이언트 모드 실행 예제
```commandline
from pyspark.sql import SparkSession

spark = SparkSession.builder \
    .appName("MySparkApp") \ 
    .master("local[1]") \  # 로컬 머신에서 실행
    .config("spark.executor.instances", 3) \  # 실행할 Executor 인스턴스 수 설정
    .config("spark.executor.cores", 1) \  # 각 Executor의 코어 수 설정
    .getOrCreate()
```

Hive Thirft 서버 클러스터 모드
```commandline
from pyspark.sql import SparkSession

spark = SparkSession.builder \
    .appName("MySparkApp") \  # 애플리케이션 이름 설정
    .config("spark.sql.warehouse.dir", "hdfs://<namenode>:<port>/user/hive/warehouse") \  # Hive warehouse 경로 설정
    .config("spark.hadoop.hive.metastore.uris", "thrift://<metastore-host>:<port>") \  # Hive metastore 주소 설정
    .config("spark.executor.instances", 3) \  # 실행할 Executor 인스턴스 수 설정
    .config("spark.executor.cores", 1) \  # 각 Executor의 코어 수 설정
    .enableHiveSupport() \  # Hive 지원 활성화
    .getOrCreate()
```