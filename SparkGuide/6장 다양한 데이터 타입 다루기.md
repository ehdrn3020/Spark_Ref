
## 6장 다양한 데이터 타입 다루기

```commandline
다양한 데이터 타입
- boolean
- 수치
- 문자열
- data와 timestamp
- null 값
- 복합 데이터
- 사용자 정의 함수
```

### 6.1 API는 어디서 찾을까
```commandline
- DataFrame은 Row 타입을 가진 Dataset이므로 결국에는 Dataset 메서드를 참조함
- org.apache.spark.sql.functions 패치지는 데이터 타입과 다양한 함수를 제공

- 분석에 사용할 DataFrame
df = spark.read.format("csv")\
    .option("header", "true")\
    .option("inferSchema", "true")\
    .load('/Users/myName/Test/Spark_ref/sparkGuide/data/2010-12-01.csv')
df.printSchema() 
```
<br/>

### 6.2 스파크 데이터 타입으로 변환하기
```commandline
from pyspark.sql.functions import lit
df.select(lit(5), lit("five"), lit(5.0))
```
<br/>

### 6.3 불리언 데이터 타입 다루기
```commandline
from pyspark.sql.functions import col
df.where("InvoiceNo <> 536365").select("InvoiceNo","Description").show(5, False)

- 중첩 조건
from pyspark.sql.functions import col, instr
priceFilter = col("UnitPrice") > 600
descripFilter = instr(df.Description, "POSTAGE") >= 1
df.where(df.StockCode.isin("DOT")).where(priceFilter|descripFilter).show()
+---------+---------+--------------+--------+-------------------+---------+----------+--------------+
|InvoiceNo|StockCode|   Description|Quantity|        InvoiceDate|UnitPrice|CustomerID|       Country|
+---------+---------+--------------+--------+-------------------+---------+----------+--------------+
|   536544|      DOT|DOTCOM POSTAGE|       1|2010-12-01 14:32:00|   569.77|      NULL|United Kingdom|
|   536592|      DOT|DOTCOM POSTAGE|       1|2010-12-01 17:06:00|   607.49|      NULL|United Kingdom|
+---------+---------+--------------+--------+-------------------+---------+----------+--------------+

- 불리언을 컬럼에 사용
df.withColumn("isExpensive", (col("StockCode") == "DOT") & (priceFilter|descripFilter))\
    .where("isExpensive")\
    .select("unitPrice", "isExpensive").show(5)
+---------+-----------+
|unitPrice|isExpensive|
+---------+-----------+
|   569.77|       true|
|   607.49|       true|
+---------+-----------+
```
<br/>

### 6.4 수치형 데이터 타입 다루기
```commandline
- 데이터 연산
from pyspark.sql.functions import expr, pow
fabQuantity = pow(col("Quantity") * col("UnitPrice"), 2) + 5
df.select("Quantity", "UnitPrice", fabQuantity.alias("realQuantity")).show(2)
+--------+---------+------------------+
|Quantity|UnitPrice|      realQuantity|
+--------+---------+------------------+
|       6|     2.55|239.08999999999997|
|       6|     3.39|          418.7156|
+--------+---------+------------------+

- 수치형 요약 통계
df.describe().show()
+-------+-----------------+------------------+--------------------+------------------+------------------+------------------+--------------+
|summary|        InvoiceNo|         StockCode|         Description|          Quantity|         UnitPrice|        CustomerID|       Country|
+-------+-----------------+------------------+--------------------+------------------+------------------+------------------+--------------+
|  count|             3108|              3108|                3098|              3108|              3108|              1968|          3108|
|   mean| 536516.684944841|27834.304044117645|                NULL| 8.627413127413128| 4.151946589446603|15661.388719512195|          NULL|
| stddev|72.89447869788873|17407.897548583845|                NULL|26.371821677029203|15.638659854603892|1854.4496996893627|          NULL|
|    min|           536365|             10002| 4 PURPLE FLOCK D...|               -24|               0.0|           12431.0|     Australia|
|    max|          C536548|              POST|ZINC WILLIE WINKI...|               600|            607.49|           18229.0|United Kingdom|
+-------+-----------------+------------------+--------------------+------------------+------------------+------------------+--------------+

- StatFunctions 패키지를 통해 다양한 통계함수 제공
- 블룸 필터링이나 스케칭 알고리즘과 같은 고급 기법과 관련된 함수 제공
df.stat.crosstab("StockCode", "Quantity").show()
```
<br/>

### 6.5 문자열 데이터 타입 다루기
```commandline
- lower 함수를 사용해 소문자로, upper 함수를 사용해 대문자로 변경
from pyspark.sql.functions import lower, upper
df.select(col("Description")
    ,lower(col("Description"))
    ,upper(col("Description"))).show(2)
+--------------------+--------------------+--------------------+
|         Description|  lower(Description)|  upper(Description)|
+--------------------+--------------------+--------------------+
|WHITE HANGING HEA...|white hanging hea...|WHITE HANGING HEA...|
| WHITE METAL LANTERN| white metal lantern| WHITE METAL LANTERN|
+--------------------+--------------------+--------------------+


- 문자열 주변의 공백을 제거하거나 추가
from pyspark.sql.functions import lit, ltrim, rtrim, rpad, lpad, trim
df.select(
    ltrim(lit(" Hellow  ")).alias("ltrim"),
    rtrim(lit(" Hellow  ")).alias("rtrim"),
    trim(lit(" Hellow  ")).alias("trim"),
    lpad(lit("Hellow"),3, " ").alias("lpad"),
    rpad(lit("Hellow"),10, " ").alias("rpad")
).show(2)
|   ltrim|  rtrim|  trim|lpad|      rpad|
+--------+-------+------+----+----------+
|Hellow  | Hellow|Hellow| Hel|Hellow    |
|Hellow  | Hellow|Hellow| Hel|Hellow    |
+--------+-------+------+----+----------+


- 정규 표현식을 regex_replace 함수를 이용해 치환
from pyspark.sql.functions import regexp_replace
regex_string = "BLACK|WHITE|RED|GREEN|BLUE"
df.select(
    col("Description"),
    regexp_replace(col("Description"), regex_string, "COLOR").alias("color_clean")
).show(2, False)
+----------------------------------+----------------------------------+
|Description                       |color_clean                       |
+----------------------------------+----------------------------------+
|WHITE HANGING HEART T-LIGHT HOLDER|COLOR HANGING HEART T-LIGHT HOLDER|
|WHITE METAL LANTERN               |COLOR METAL LANTERN               |
+----------------------------------+----------------------------------+


- 문자열 치환을 translate 함수를 이용
from pyspark.sql.functions import translate
df.select(col("Description"), translate(col("Description), "LEFT", 1337).show(2)
df.select(
    col("Description"), 
    translate(col("Description"), "LEFT", "1337")
).where(instr(col("Description"), "WHITE")>=1)
.show(2,False)
+----------------------------------+----------------------------------+
|Description                       |translate(Description, LEFT, 1337)|
+----------------------------------+----------------------------------+
|WHITE HANGING HEART T-LIGHT HOLDER|WHI73 HANGING H3AR7 7-1IGH7 HO1D3R|
|WHITE METAL LANTERN               |WHI73 M37A1 1AN73RN               |
+----------------------------------+----------------------------------+


- 동적으로 인수의 개수가 변하는 상황 ( locate )
from pyspark.sql.functions import expr, locate
simpleColors = ["black", "white", "red", "green", "blue"]
def color_locator(column, color_string):
    return locate(color_string.upper(), column).cast("boolean").alias("is_"+color_string)

selectedColumns = [color_locator(col("Description"), c) for c in simpleColors]
selectedColumns.append(expr("*"))
df.select(*selectedColumns).where(expr("is_white Or is_red")).show(3,False)
+--------+--------+------+--------+-------+---------+---------+----------------------------------+--------+-------------------+---------+----------+--------------+
|is_black|is_white|is_red|is_green|is_blue|InvoiceNo|StockCode|Description                       |Quantity|InvoiceDate        |UnitPrice|CustomerID|Country       |
+--------+--------+------+--------+-------+---------+---------+----------------------------------+--------+-------------------+---------+----------+--------------+
|false   |true    |false |false   |false  |536365   |85123A   |WHITE HANGING HEART T-LIGHT HOLDER|6       |2010-12-01 08:26:00|2.55     |17850.0   |United Kingdom|
|false   |true    |false |false   |false  |536365   |71053    |WHITE METAL LANTERN               |6       |2010-12-01 08:26:00|3.39     |17850.0   |United Kingdom|
|false   |true    |true  |false   |false  |536365   |84029E   |RED WOOLLY HOTTIE WHITE HEART.    |6       |2010-12-01 08:26:00|3.39     |17850.0   |United Kingdom|
+--------+--------+------+--------+-------+---------+---------+----------------------------------+--------+-------------------+---------+----------+--------------+
```