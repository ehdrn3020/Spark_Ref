
## 9장 데이터소스

```commandline
스파크 핵심 외부 데이터소스
- CSV
- JSON
- PARQUET
- ORC
- JDBC/ODBC
- TEXT FILE
- 파일 옵션
    ㄴ https://spark.apache.org/docs/latest/sql-data-sources-csv.html
```
<br/>

### 9.1 데이터소스 API 구조
#### 읽기 모드
```commandline
데이터 읽기 핵심 구조
- DataFrameReader.format(...).option("key","value").schema(...).load()
- DataFrameReader는 SparkSession의 read 속성으로 접근

읽기 모드 지정 값
- 포맷
- 스키마
- 읽기 모드 
    ㄴ permissive : 오류레코드 null 설정
    ㄴ dropMalformed : 오류레코드 제거
    ㄴ failFast : 오류레코드시 만나면 즉시 종료
- 옵션

예제
spark.read.format("csv")
    .option("mode","FAILFAST")
    .option("inferSchema","true")
    .option("path","path/to/file(s)")
    .schema(someSchema)
    .load()    
```

#### 쓰기 모드
```commandline
데이터 쓰기 핵심 구조
- DataFrameWriter.format(...).option(...).partitionBy(...).bucketBy(...).sortBy(...).save()
- DataFrameWriter는 DataFrame의 write 속성으로 접근

쓰기 모드 지정 값
- 포맷
- 옵션
- 저장 모드
    ㄴ append : 해당 경로에 이미 존재하는 파일 목록에 결과 파일을 추가
    ㄴ overwrite : 이미 존재하는 모든 데이터를 덮어씀
    ㄴ errorIfExists : 해당 경로에 파일이 존재하면 오류 발생
    ㄴ ignore : 해당 경로에 데이터 파일이 존재하면 아무런 처리하지 않음

예제
dataframe.write.format("csv")
    .option("mode","OVERWRITE")
    .option("dataFormat","yyyy-MM-dd")
    .option("path","path/to/file(s)")
    .save()
```
<br/>

### 9.2 CSV 파일
#### CSV 옵션
<img src="./image/9-1.png" width="650">

```commandline
# CSV 읽기
csvFile = spark.read.format("csv")
    .option("header","true")
    .option("mode","FAILFAST")
    .option("inferSchema","true")
    .load("some/path/to/file.csv")

# CSV 쓰기
csvFile.write.format("csv")
    .option("mode","OVERWRITE")
    .option("sep","\t")
    .save("path/to/file(s)")
```
<br/>

### 9.3 Json 파일
```commandline
- 구조화되어 있고, 최소한의 기본 데이터 타입이 존재
```
#### Json 옵션
<img src="./image/9-2.png" width="650">
<br/>

### 9.4 Parquet 파일
```commandline
- 개별 컬럼을 기준으로 읽어 저장공간을 절약하고, 컬럼 기반의 압축 기능 제공
- 스파크 기본 파일 포맷
- 복합 데이터 타입을 지원 ( csv에서는 배열 사용못함 )
- 스파크와 호환이 잘 되기에 옵션이 2개만 존재
```
#### Parquet 옵션
<img src="./image/9-3.png" width="650">
<br/>

### 9.5 ORC 파일
```commandline
- 컬럼 기반의 파일 포맷
- 대규모 스트리밍 읽기에 최적화, 로우를 신속하게 찾아낼 수 있는 기능
- parquet는 spark에 최적화, orc는 hive에 최적화
```
<br/>

### 9.6 SQL Database
```commandline
- 접속 / 인증 및 네트워크 관련 옵션이 필요
- 스파크 classpath에 Database JDBC Driver를 추가하고 적절한 jar 파일을 제공
    ㄴ 예제) ./bin/spark-shell --driver-class-path postgresql-9.4.1.jar --jars postgresql-9.4.1.jar
```
#### JDBC 옵션
<img src="./image/9-4.png" width="650">
<br/>


