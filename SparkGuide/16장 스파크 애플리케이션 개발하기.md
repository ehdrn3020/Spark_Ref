
## 16장 스파크 애플리케이션 개발하기


### 16.1 파이썬 애플리케이션 작성하기

```commandline
spark-sumit을 통한 spark 실행
- 명령어 : spark-submit --master local SparkGuide/example/16_1.py
- 관련 코드
from __future__ import print_function

if __name__ == '__main__':
    from pyspark.sql import SparkSession
    spark = SparkSession.builder \
        .master("local") \
        .appName("Word Count") \
        .config("spark.some.config.option", "some-value") \
        .getOrCreate()

    print(spark.range(5000).where("id>500").selectExpr("sum(id)").collect())
```

### 16.2 스파크 애플리케이션 테스트
```commandline
스파크 애플리케이션에서 테스트 해야 할 내용과 테스트 편리성을 높여주는 코드 구성
- 입력 데이터에 대한 유연성
- 비즈니스 로직 변경에 대한 유연성
- 결과의 유연성과 원자성

테스크 코드 작성시 고려사항
- SparkSession 관리하기
    - SparkSession을 한 번만 초기화하고 런타임 환경에서 함수와 클래스에 전달하는 방식을 사용
- 테스트 코드용 스파크 API 선정

단위 테스트 프레임워크 연결하기 
- JUnit 또는 ScalaTst 사용 ( + Pytest )
- 테스트마다 SaprkSession을 생성하고 제거하도록 설정

데이터소스 연결하기
- 운영환경의 데이터소스에 접근 지양
```

### 16.4 애플리케이션 시작하기 
```commandline
실행 커맨트
./bin/spark-submit \
    --class <main class> \
    --master <spark master URL> \
    --deploy-mode <deploy mode> \
    --conf <key>=<value> \
    ... another options
    <Jar file or Script file> \
    [인수 : arg_1, arg_2]
    
예제
./bin/spark-submit \
    --class org.apache.spark.examples.SparkPi \
    --master spark://123.123.123.123:7077 \
    --executor-memory 20G \
    --total-executor-cores 100 \
    replace/with/path/to/examples.jar \
    1000


Local 환경에서 master-worker 실행
    
# 설치 경로 확인
brew --prefix apache-spark
# 로컬일 경우 22 포트 사용하지 않음
export SPARK_NO_DAEMONIZE=true
# master - slave 실행 
/opt/homebrew/Cellar/apache-spark/3.5.1/libexec/sbin/sbin/start-master.sh
/opt/homebrew/Cellar/apache-spark/3.5.1/libexec/sbin/sbin/start-worker.sh spark://AL12341234.local:7077
```

### 16.5 애플리케이션 환경 설정하기
